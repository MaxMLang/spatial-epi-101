[
  {
    "objectID": "modules/module5-modeling.html",
    "href": "modules/module5-modeling.html",
    "title": "Module 5: Analyzing Spatial Clustering",
    "section": "",
    "text": "This module introduces several methods for formally exploring and testing for clustering in spatial data. A key question in spatial epidemiology is whether disease cases are clustered together, and if so, where these clusters are located. Identifying non-random patterns can provide crucial insights into disease transmission and help target public health interventions.\nWe will explore methods for two main types of spatial data:\nAreal Data: Analyzing rates or counts within defined boundaries (e.g., states or districts).\nPoint Pattern Data: Analyzing the specific locations of individual case events."
  },
  {
    "objectID": "modules/module5-modeling.html#setup-loading-packages",
    "href": "modules/module5-modeling.html#setup-loading-packages",
    "title": "Module 5: Analyzing Spatial Clustering",
    "section": "5.1 Setup: Loading Packages",
    "text": "5.1 Setup: Loading Packages\nFirst we will load the necessary R packages. These include sf for handling spatial data, spdep for spatial dependence analysis, spatstat for point pattern analysis, and smacpod for scan statistics.\n\n\nShow/Hide Code\n# install.packages(\"spdep\")\n# install.packages(\"spatstat.geom\")\n# install.packages(\"spatstat.explore\")\n# install.packages(\"smacpod\")\n\n\n\n\nShow/Hide Code\n# Core data handling and visualization\nlibrary(sf)\nlibrary(tmap)\nlibrary(dplyr)\n\n# For spatial autocorrelation tests\nlibrary(spdep)\n\n# For point pattern analysis\nlibrary(spatstat.geom)\nlibrary(spatstat.explore)\nlibrary(smacpod)\n\n# Data Packages\nlibrary(spData)"
  },
  {
    "objectID": "modules/module5-modeling.html#part-1-autocorrelation-in-areal-data-morans-i",
    "href": "modules/module5-modeling.html#part-1-autocorrelation-in-areal-data-morans-i",
    "title": "Module 5: Analyzing Spatial Clustering",
    "section": "5.2 Part 1: Autocorrelation in Areal Data (Moran’s I)",
    "text": "5.2 Part 1: Autocorrelation in Areal Data (Moran’s I)\nWhen you have data aggregated into polygons (like states), you often want to know if areas with high values are located near other areas with high values. This is called spatial autocorrelation. The most common statistical test for this is Moran’s I.\n\nGlobal Moran’s I: Tells you if there is a general trend of clustering somewhere in your overall study area.\nLocal Moran’s I (LISA): Identifies the location of specific “hotspots” (High-High clusters), “coldspots” (Low-Low clusters), and spatial outliers.\n\n\nWorked Example: Global Clustering of Income in the USA\nWe will use the us_states dataset from the spData package to test for global spatial clustering in median income.\n\n\nShow/Hide Code\n# 1. Load the data\ndata(\"us_states\", package = \"spData\") \ndata(\"us_states_df\", package = \"spData\") \nus_states &lt;- us_states %&gt;% \n  left_join(us_states_df, by = c( \"NAME\" =\"state\"))\n\n# 2. Define Neighbors\n# We first determine which states are neighbors using \"queen\" contiguity,\n# meaning they are neighbors if they share a border or a corner.\nstate_neighbors &lt;- poly2nb(us_states, queen = TRUE)\n\n# 3. Assign Spatial Weights\n# We convert the neighbor list into a weights matrix.\nstate_weights &lt;- nb2listw(state_neighbors, style = \"W\")\n\n# 4. Run the Global Moran's I Test\n# We test for autocorrelation in the median income variable.\nmoran.test(us_states$median_income_15, listw = state_weights)\n\n\n\n    Moran I test under randomisation\n\ndata:  us_states$median_income_15  \nweights: state_weights    \n\nMoran I statistic standard deviate = 4.5336, p-value = 2.899e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.407524868      -0.020833333       0.008927291 \n\n\nInterpretation: The output shows a very small p-value, indicating that the spatial pattern of median income is not random. The positive Moran I statistic confirms that states with similar median incomes are clustered together.\n\n\nWorked Example: Identifying Local Income Clusters (LISA)\nNow we will use Local Moran’s I to find the specific locations of these income clusters.\n\n\nShow/Hide Code\n# 1. Calculate Local Moran's I for each state\nlisa_results &lt;- localmoran(us_states$median_income_15, listw = state_weights)\n\n# 2. Create a Moran Scatter Plot\n# This plots the income in a state vs the average income of its neighbors.\n# The four quadrants represent the cluster types: High-High, Low-Low, High-Low, Low-High.\nmoran.plot(us_states$median_income_15, listw = state_weights,\n           xlab = \"Median Income\", ylab = \"Spatially Lagged Median Income\")\n\n\n\n\n\n\n\n\n\nShow/Hide Code\n# 3. Map the Significant Clusters\n# Let's identify the cluster type for each state and add it to our data.\nus_states$lisa_I &lt;- lisa_results[, \"Ii\"]\nus_states$lisa_p_value &lt;- lisa_results[, \"Pr(z != E(Ii))\"]\nus_states$lisa_quadrant &lt;- attr(lisa_results, \"quadr\")[, \"pysal\"]\n\n# Keep only the statistically significant clusters (p &lt; 0.05)\nsignificant_clusters &lt;- us_states[lisa_results[, \"Pr(z != E(Ii))\"] &lt; 0.05, ]\n\n# Create the map\ntm_shape(us_states) + tm_polygons(col = \"gray85\", border.col = \"white\") +\n  tm_shape(significant_clusters) +\n    tm_polygons(\"lisa_quadrant\", palette = \"viridis\", title = \"Cluster Type\") +\n  tm_layout(main.title = \"Significant Local Clusters of Median Income\")\n\n\n\n\n\n\n\n\n\nOf course. Here is a complete, self-contained example using simulated data to demonstrate the concepts of Ripley’s K-function and the spatial scan statistic.\nThis example generates a “clustered” set of points for our cases and a “random” set of points for our controls, allowing us to test the methods."
  },
  {
    "objectID": "modules/module5-modeling.html#part-2-analyzing-spatial-point-patterns-with-simulated-data",
    "href": "modules/module5-modeling.html#part-2-analyzing-spatial-point-patterns-with-simulated-data",
    "title": "Module 5: Analyzing Spatial Clustering",
    "section": "5.3 Part 2: Analyzing Spatial Point Patterns with Simulated Data",
    "text": "5.3 Part 2: Analyzing Spatial Point Patterns with Simulated Data\nWhen you have the precise locations of individual events, you can use point pattern analysis to see if they are clustered. Since the original datasets are unavailable, we’ll generate our own data to illustrate the methods.\nFirst we need to load the spatstat library. We will then generate two sets of points within a unit square:\n\nCases: A clustered point pattern. We’ll use rMatClust to simulate 5 parent events that generate a total of 100 child points clustered around them.\nControls: A completely random point pattern using rpoispp.\n\n\n\nShow/Hide Code\n# Load the required library for spatial analysis\nlibrary(spatstat)\n\n# Step 1: Define the observation window (a simple unit square)\nwin &lt;- owin(c(0, 1), c(0, 1))\n\n# Step 2: Generate clustered \"case\" data\n# We'll simulate a Matern Cluster process with 5 parent points,\n# a cluster radius of 0.1, and an average of 20 children per parent.\ncases_ppp &lt;- rMatClust(kappa = 5, scale = 0.1, mu = 20, win = win)\n# Let's label these as \"disease\"\nmarks(cases_ppp) &lt;- factor(\"case\")\n\n# Step 3: Generate random \"control\" data\n# We'll simulate a random Poisson process with 200 points.\ncontrols_ppp &lt;- rpoispp(lambda = 200, win = win)\n# Let's label these as \"control\"\nmarks(controls_ppp) &lt;- factor(\"control\")\n\n# Step 4: Combine cases and controls into a single marked ppp object\n# The 'superimpose' function is perfect for this.\nall_points_ppp &lt;- superimpose(cases_ppp, controls_ppp)\n\n# Let's verify the combined object\nprint(\"Combined PPP object with cases and controls:\")\n\n\n[1] \"Combined PPP object with cases and controls:\"\n\n\nShow/Hide Code\nprint(all_points_ppp)\n\n\nMarked planar point pattern: 299 points\nMultitype, with levels = case, control \nwindow: rectangle = [0, 1] x [0, 1] units\n\n\nShow/Hide Code\n# Plot the generated data using the recommended 'spatstat' arguments\n\nplot(all_points_ppp,\n     main = \"Simulated Cases (Clustered) and Controls (Random)\",\n     cols = c(\"red\", \"black\"),   \n     chars = c(16, 2),           \n     cex = 0.8)\n\n\n\n\n\n\n\n\n\n\nMethod 1: Ripley’s K-function\nThe K-function summarizes spatial dependence over a range of distances. We compare the observed K-function from our data to what we would expect under complete spatial randomness (CSR).\nWe will analyze only the “case” points to see if they are clustered.\n\n\nShow/Hide Code\n# Calculate the K-function for our simulated cases\n# We create a confidence envelope from 99 random simulations to test significance.\nk_result &lt;- envelope(cases_ppp, Kest, nsim = 99)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\nShow/Hide Code\n# Plot the result\nplot(k_result, main = \"Ripley's K-function for Simulated Cases\")\n\n\n\n\n\n\n\n\n\nInterpretation: The plot shows the theoretical K-function for a random pattern (red dashed line) and a grey confidence envelope. The solid black line is the observed K-function for our simulated case data. Because the black line rises significantly above the grey envelope, it correctly indicates that our generated points are more clustered than would be expected by random chance.\n\n\nMethod 2: Spatial Scan Statistic\nThe spatial scan statistic is a method used to detect and evaluate the location and significance of spatial clusters. It works by moving a circular window across the map and comparing the rate of cases to controls inside the circle versus outside.\nWe use our combined all_points_ppp object which contains both cases and controls.\n\n\nShow/Hide Code\nscan_result_small &lt;- spscan.test(all_points_ppp, nsim = 99, case = \"case\", maxd = 0.1)\n\n# Print the new results\nprint(scan_result_small)\n\n\nmethod: circular scan\ndistance upperbound:  0.1\nrealizations: 99\n\n\nShow/Hide Code\n# --- Plotting the New Result ---\n\n# First, plot the points using the corrected plotting code from before\nplot(all_points_ppp,\n     main = \"Most Likely Cluster\",\n     cols = c(\"red\", \"black\"),\n     chars = c(16, 2),\n     cex = 0.8)\n\n# Now, add the new, smaller cluster circle to the plot\nplot(scan_result_small, add = TRUE, border = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nInterpretation: The blue circle on the map shows the location and size of the statistically significant cluster of “cases” relative to “controls”. As expected, the algorithm has successfully identified one of the clusters we intentionally generated in our simulation."
  },
  {
    "objectID": "modules/module3-remote-sensing.html",
    "href": "modules/module3-remote-sensing.html",
    "title": "Module 3: Remote Sensing Data for Environmental Epidemiology",
    "section": "",
    "text": "In the previous modules, we learned how to handle and analyze existing vector and raster datasets. But where does much of our environmental raster data come from? The answer is Remote Sensing (RS)—the science of obtaining information about the Earth’s surface from a distance, typically using satellites.\nThis module provides a primer on the fundamentals of remote sensing and demonstrates how to process satellite imagery in R to derive key environmental indices used in epidemiology. Understanding the source and nature of your data is critical for interpreting it correctly and being aware of its limitations.\nBy the end of this notebook, you will be able to:"
  },
  {
    "objectID": "modules/module3-remote-sensing.html#setup-loading-packages-and-data",
    "href": "modules/module3-remote-sensing.html#setup-loading-packages-and-data",
    "title": "Module 3: Remote Sensing Data for Environmental Epidemiology",
    "section": "3.1 Setup: Loading Packages and Data",
    "text": "3.1 Setup: Loading Packages and Data\nWe will use our standard suite of packages. We will also need the spDataLarge package, which contains a sample Landsat satellite image perfect for our exercises.\nIf you have not installed spDataLarge before, please do so now. Note: it is a large package.\n\n\nShow/Hide Code\n# This command only needs to be run once.\n# install.packages(\"spDataLarge\")\n\n\nNow, let’s load our required libraries.\n\n\nShow/Hide Code\n# Core Packages for spatial data handling and plotting\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(dplyr)\n\n# Data Package with a sample satellite image\nlibrary(spDataLarge)"
  },
  {
    "objectID": "modules/module3-remote-sensing.html#fundamentals-of-remote-sensing",
    "href": "modules/module3-remote-sensing.html#fundamentals-of-remote-sensing",
    "title": "Module 3: Remote Sensing Data for Environmental Epidemiology",
    "section": "3.2 Fundamentals of Remote Sensing",
    "text": "3.2 Fundamentals of Remote Sensing\nThis section is conceptual but essential for understanding the data we’ll be working with.\n\nThe Four Resolutions\nThe quality and characteristics of satellite data are defined by four types of resolution: Spatial, Temporal, Spectral, and Radiometric.\n\n\nCommon Satellite Systems for Public Health\n\nLandsat Program (NASA/USGS): The workhorse of environmental monitoring.\nSentinel Program (ESA Copernicus): A newer fleet of satellites providing high-resolution data.\nMODIS (NASA): This sensor provides coarse spatial resolution imagery but has a very high temporal resolution (daily global coverage)."
  },
  {
    "objectID": "modules/module3-remote-sensing.html#deriving-and-interpreting-environmental-indices-in-r",
    "href": "modules/module3-remote-sensing.html#deriving-and-interpreting-environmental-indices-in-r",
    "title": "Module 3: Remote Sensing Data for Environmental Epidemiology",
    "section": "3.3 Deriving and Interpreting Environmental Indices in R",
    "text": "3.3 Deriving and Interpreting Environmental Indices in R\nEnvironmental indices are calculated by combining the reflectance values from different spectral bands. This allows us to enhance specific features, like vegetation or water, that are not obvious in a standard visible-light image.\n\nLoading Satellite Data\nWe will use a sample Landsat image that comes with the spDataLarge package. This is a multi-band SpatRaster.\n\n\nShow/Hide Code\n# Get the file path to the sample landsat image from the package\nlandsat_path &lt;- system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\n\n# Load the image using terra::rast()\nlandsat &lt;- rast(landsat_path)\n\n# Inspect the object to see its properties (dimensions, resolution, CRS)\nprint(landsat)\n\n\nclass       : SpatRaster \nsize        : 1428, 1128, 4  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \nsource      : landsat.tif \nnames       : landsat_1, landsat_2, landsat_3, landsat_4 \nmin values  :      7550,      6404,      5678,      5252 \nmax values  :     19071,     22051,     25780,     31961 \n\n\nShow/Hide Code\n# Look at the names of the bands. \n# This specific file includes 7 bands from a Landsat 8 scene.\n# Band 1 = Blue, Band 2 = Green, Band 3 = Red, Band 4 = NIR, etc.\nnames(landsat)\n\n\n[1] \"landsat_1\" \"landsat_2\" \"landsat_3\" \"landsat_4\"\n\n\nThis sample landsat.tif image is a real satellite excerpt covering the area around Zion National Park in Utah, USA. We can visually confirm this by plotting the park’s boundary over the raster image.\n\n\nShow/Hide Code\n# Load the Zion National Park boundary to prove the location\nzion_gpkg_path &lt;- system.file(\"vector/zion.gpkg\", package = \"spDataLarge\")\nzion_boundary &lt;- st_read(zion_gpkg_path, layer = \"zion\")\n\n\nReading layer `zion' from data source \n  `/Users/max/Library/Caches/org.R-project.R/R/renv/cache/v5/macos/R-4.4/aarch64-apple-darwin20/spDataLarge/2.1.2/4b90966547c4fd3c5ffecbf6ad608be5/spDataLarge/vector/zion.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 302903.1 ymin: 4112244 xmax: 334735.5 ymax: 4153087\nProjected CRS: UTM Zone 12, Northern Hemisphere\n\n\nShow/Hide Code\n# Create a true-color composite plot with the boundary overlaid\n# For a true-color image, we map the Red, Green, and Blue bands to the RGB channels.\n# For this dataset: Red = band 3, Green = band 2, Blue = band 1.\ntm_shape(landsat) +\n  tm_rgb(r = 3, g = 2, b = 1, stretch = TRUE) +\ntm_shape(zion_boundary) +\n  tm_borders(col = \"black\", lwd = 2) +\n  tm_layout(main.title = \"Zion National Park Boundary over Landsat Image\", \n            main.title.position = \"center\")\n\n\n\n\n\n\n\n\n\nAs you can see, the raster image aligns perfectly with the park boundary. For this dataset, the standard band order we need for our analysis is:\n\nLayer 2: Green\nLayer 3: Red\nLayer 4: Near-Infrared (NIR)\n\nWe will select these bands using their position number with [[...]].\n\n\nNormalized Difference Vegetation Index (NDVI)\nNDVI is a standardized index of vegetation greenness and health. The formula is: \\[NDVI = \\frac{(NIR - Red)}{(NIR + Red)}\\]\nWorked Example: Calculating NDVI\n\n\nShow/Hide Code\n# Assign the Near-Infrared (NIR) and Red bands to variables for clarity\n# Based on our data: NIR is the 4th band, Red is the 3rd band.\nnir &lt;- landsat[[4]]\nred &lt;- landsat[[3]]\n\n# Apply the NDVI formula using raster algebra\nlandsat_ndvi &lt;- (nir - red) / (nir + red)\n\n# Plot the result using tmap\n# The \"RdYlGn\" palette is a good choice for NDVI, with green indicating healthier vegetation.\ntm_shape(landsat_ndvi) +\n  tm_raster(palette = \"RdYlGn\", title = \"NDVI\") +\n  tm_layout(main.title = \"Normalized Difference Vegetation Index (NDVI)\")\n\n\n\n\n\n\n\n\n\n\n\nNormalized Difference Water Index (NDWI)\nNDWI is used to delineate open water features. A common formula is: \\[NDWI = \\frac{(Green - NIR)}{(Green + NIR)}\\]\nWorked Example: Calculating NDWI\n\n\nShow/Hide Code\n# Assign the Green and NIR bands to variables\n# Based on our data: Green is the 2nd band, NIR is the 4th band.\ngreen &lt;- landsat[[2]]\nnir &lt;- landsat[[4]]\n\n# Apply the NDWI formula using raster algebra\nlandsat_ndwi &lt;- (green - nir) / (green + nir)\n\n# Plot the result\n# The \"-RdYlBu\" palette (reversed) is used so high values (water) appear blue.\ntm_shape(landsat_ndwi) +\n  tm_raster(palette = \"-RdYlBu\", title = \"NDWI\") +\n  tm_layout(main.title = \"Normalized Difference Water Index (NDWI)\")\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1: Calculating Vegetation Indices\nNow it’s your turn to practice calculating indices from the same landsat object.\n\nTask 1Try it yourself!Solution\n\n\nThe Soil-Adjusted Vegetation Index (SAVI) is a modification of NDVI that attempts to correct for the influence of soil brightness. The formula is: \\[SAVI = \\frac{(NIR - Red)}{(NIR + Red + L)} \\times (1 + L)\\] Using the landsat raster, calculate SAVI. Assume the soil brightness correction factor L is 0.5.\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# First, ensure the landsat object from the worked example is available\n# (This code is repeated to make the solution self-contained)\nlandsat_path &lt;- system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nlandsat &lt;- rast(landsat_path)\n\n# Assign bands to variables using their position number\nnir &lt;- landsat[[4]]\nred &lt;- landsat[[3]]\nL &lt;- 0.5 # Soil brightness correction factor\n\n# Calculate SAVI using the provided formula\nlandsat_savi &lt;- ((nir - red) / (nir + red + L)) * (1 + L)\n\n# Plot your result\ntm_shape(landsat_savi) + tm_raster(palette=\"RdYlGn\", title=\"SAVI\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 2Try it yourself!Solution\n\n\nCompare your SAVI map with the NDVI map we created earlier. Use tmap_arrange() to view them side-by-side. Do you notice any subtle differences?\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Re-create all necessary objects to make this chunk self-contained\nlandsat_path &lt;- system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nlandsat &lt;- rast(landsat_path)\nnir &lt;- landsat[[4]]\nred &lt;- landsat[[3]]\nL &lt;- 0.5\n\n# Re-calculate NDVI \nlandsat_ndvi &lt;- (nir - red) / (nir + red)\n\n# Re-calculate SAVI\nlandsat_savi &lt;- ((nir - red) / (nir + red + L)) * (1 + L)\n\n# Create the two separate tmap objects\nmap_ndvi &lt;- tm_shape(landsat_ndvi) + tm_raster(palette = \"RdYlGn\", title = \"NDVI\") + tm_layout(title=\"NDVI\")\nmap_savi &lt;- tm_shape(landsat_savi) + tm_raster(palette=\"RdYlGn\", title = \"SAVI\") + tm_layout(title=\"SAVI\")\n\n# Arrange them side-by-side for comparison\ntmap_arrange(map_ndvi, map_savi)"
  },
  {
    "objectID": "modules/module3-remote-sensing.html#application-to-vector-borne-disease-ecology",
    "href": "modules/module3-remote-sensing.html#application-to-vector-borne-disease-ecology",
    "title": "Module 3: Remote Sensing Data for Environmental Epidemiology",
    "section": "3.4 Application to Vector-Borne Disease Ecology",
    "text": "3.4 Application to Vector-Borne Disease Ecology\nThe indices we’ve calculated serve as proxies for environmental conditions that favor or inhibit disease transmission.\n\nMalaria: High NDWI can indicate mosquito breeding sites. High NDVI near settlements may indicate resting sites for adult mosquitoes. Land Surface Temperature (LST) is critical, as the development of the Plasmodium parasite inside the mosquito is highly temperature-dependent.\nSchistosomiasis: NDWI is used to map the freshwater habitats of the intermediate host snails. NDVI can indicate the presence of aquatic vegetation that snails use for food and shelter. LST affects snail survival and the development of schistosome larvae."
  },
  {
    "objectID": "modules/module6-resources.html",
    "href": "modules/module6-resources.html",
    "title": "Module 6: References and Further Reading",
    "section": "",
    "text": "This final module steps back from specific methods to focus on the broader principles of conducting high-quality, reliable, and ethical geospatial research. It also provides a curated list of resources to support your continued learning journey in spatial epidemiology.\nBy the end of this notebook, you will have access to: 1. Links to the excellent online courses that inspired this one. 2. A list of key textbooks for deepening your theoretical and practical knowledge. 3. Direct links to the documentation for all the R packages used in this course. 4. An overview of best practices for ensuring your work is reproducible and ethical."
  },
  {
    "objectID": "modules/module6-resources.html#course-recommendations-and-key-resources",
    "href": "modules/module6-resources.html#course-recommendations-and-key-resources",
    "title": "Module 6: References and Further Reading",
    "section": "6.1 Course Recommendations and Key Resources",
    "text": "6.1 Course Recommendations and Key Resources\nThis course was developed with inspiration from several outstanding, publicly available resources.\n\nSpatial epidemiology in R by Hugh Sturrock A comprehensive online course covering a wide range of topics in spatial epidemiology, from which some concepts and datasets in this course were adapted.\nThe Epidemiologist R Handbook An excellent, practical guide for using R in epidemiology. The “GIS Basics” chapter provides a great overview of handling spatial data in R"
  },
  {
    "objectID": "modules/module6-resources.html#essential-textbooks-for-further-learning",
    "href": "modules/module6-resources.html#essential-textbooks-for-further-learning",
    "title": "Module 6: References and Further Reading",
    "section": "6.2 Essential Textbooks for Further Learning",
    "text": "6.2 Essential Textbooks for Further Learning\nTo dive deeper into the theory and application of spatial analysis, the following books are considered foundational texts in the field.\n\nGeocomputation with R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow. This is the modern, definitive guide to doing spatial analysis in R, with a focus on the sf, terra, and tmap packages. The entire book is available online for free.\nApplied Spatial Data Analysis with R, 2nd edition by Roger S. Bivand, Edzer Pebesma, and Virgilio Gómez-Rubio. This book provides a deep, rigorous dive into the statistical theory behind many spatial methods, including the use of the spdep package.\nSpatial Data Science: With Applications in R by Edzer Pebesma and Roger Bivand. An updated text from the leading figures in the R-spatial community, reflecting the most current approaches and packages."
  },
  {
    "objectID": "modules/module6-resources.html#core-r-packages-used-in-this-course",
    "href": "modules/module6-resources.html#core-r-packages-used-in-this-course",
    "title": "Module 6: References and Further Reading",
    "section": "6.3 Core R Packages Used in This Course",
    "text": "6.3 Core R Packages Used in This Course\nThis course would not be possible without the incredible ecosystem of open-source R packages developed and maintained by the community. Below are links to the official documentation for the primary packages we used.\n\nData Handling and Manipulation\n\nsf: The modern standard for working with vector data (points, lines, polygons) in R.\nterra: The modern, high-performance package for working with raster data.\ndplyr: A core part of the tidyverse, providing a powerful grammar for data manipulation that works seamlessly with sf objects.\n\n\n\nData Access\n\nspData: Contains the diverse sample datasets used throughout this course, such as us_states and world.\nspDataLarge: A companion package containing larger datasets, including the landsat.tif satellite image.\ngeodata: A package for downloading common global datasets like country boundaries from GADM and climate data from WorldClim.\n\n\n\nVisualization\n\ntmap: A dedicated package for creating beautiful and flexible static and interactive thematic maps.\nrayshader: A package for creating beautiful 2D and 3D data visualizations, especially for mapping and elevation data.\n\n\n\nSpatial Statistics & Validation\n\nspdep: A foundational package for analyzing spatial dependence, including tools for creating neighbor weights and running tests like Moran’s I.\nspatstat: A comprehensive suite of tools for point pattern analysis, including Ripley’s K-function.\ncaret: A general-purpose package for machine learning and model training that provides excellent tools for creating confusion matrices and calculating accuracy statistics.\npROC: A specialized package for generating, analyzing, and plotting ROC curves.\nsmacpod: A package that implements spatial scan statistics."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "An Introduction to Spatial Epidemiology using R"
  },
  {
    "objectID": "about.html#about-this-page",
    "href": "about.html#about-this-page",
    "title": "About",
    "section": "About This Page",
    "text": "About This Page\nThis workbook provides a comprehensive introduction to spatial epidemiology, designed to equip you with the essential skills needed to analyze, visualize, and interpret spatial health data using R. Through hands-on exercises and real-world examples, you’ll learn how to apply spatial statistical methods to understand disease patterns, identify clusters, and explore environmental determinants of health.\n\nWhat You’ll Learn\nBy the end of this workbook, you will be able to handle complex spatial datasets, create compelling visualizations, conduct spatial statistical analyses, and interpret results in the context of public health research. The curriculum emphasizes reproducible research practices and ethical considerations when working with health data.\n\n\nWho This Page Is For\nIdeal for:\nEpidemiologists seeking to incorporate spatial methods, Graduate students in public health, geography, or related fields, Data scientists working with health or environmental data.Anyone interested in understanding spatial patterns in health outcomes\nPrerequisites:\n\nBasic knowledge of R programming, Fundamental understanding of epidemiological concepts, Familiarity with basic statistical methods, No prior experience with spatial analysis required"
  },
  {
    "objectID": "about.html#learning-approach",
    "href": "about.html#learning-approach",
    "title": "About",
    "section": "Learning Approach",
    "text": "Learning Approach\nThe workbook follows a practical, hands-on approach where theory is immediately applied through coding exercises and case studies. Each module builds upon previous knowledge, progressing from basic spatial data handling to advanced statistical modeling techniques. You’ll work with real datasets and tackle authentic public health challenges throughout the workbook.\n\nQuestions about the workbook?\nContact: max.lang[at]stx.ox.ac.uk"
  },
  {
    "objectID": "index.html#welcome-to-the-workbook",
    "href": "index.html#welcome-to-the-workbook",
    "title": "spatial-epi-101",
    "section": "Welcome to the Workbook",
    "text": "Welcome to the Workbook\nThis workbook provides a comprehensive introduction to the principles and practices of spatial epidemiology. You will learn how to leverage the power of R to analyze, visualize, and interpret spatial health data, gaining critical skills for public health research and practice."
  },
  {
    "objectID": "index.html#workbook-modules",
    "href": "index.html#workbook-modules",
    "title": "spatial-epi-101",
    "section": "Workbook Modules",
    "text": "Workbook Modules\n\n\n\n\nModule 1: Mastering R for Spatial Data\nBuild a rock-solid foundation in handling, manipulating, and visualizing vector and raster data using the essential sf, terra, and tmap packages.\n\n\n\n\n\n\nModule 2: Advanced Spatial Data Operations\nLearn the core data wrangling techniques of spatial analysis, including spatial joins, buffering, and extracting raster values for defined zones.\n\n\n\n\n\n\nModule 3: Remote Sensing for Epidemiology\nProcess satellite imagery to derive key environmental variables like NDVI and NDWI, and understand their application in modeling vector-borne diseases.\n\n\n\n\n\n\nModule 4: Validation and Accuracy Assessment\nLearn how to validate your spatial models and maps against ground-truth data using confusion matrices, ROC curves, and other key statistical metrics.\n\n\n\n\n\n\nModule 5: Analyzing Spatial Clustering\nMove from visualization to formal testing. This module introduces spatial statistics like Moran’s I and Ripley’s K-function to identify disease clusters and hotspots.\n\n\n\n\n\n\nModule 6: Resources & Best Practices\nA curated list of key textbooks, R packages, and online resources to continue your learning journey. This module also covers principles of ethical and reproducible research."
  },
  {
    "objectID": "modules/module4-validation.html",
    "href": "modules/module4-validation.html",
    "title": "Module 4: Validating Remote Sensed Data with Ground-Truth Observations",
    "section": "",
    "text": "In Module 3, we successfully transformed raw satellite imagery into meaningful environmental variables like NDVI. We now have maps that suggest where vegetation is healthy or where water bodies exist. But a crucial question remains: How accurate are these maps? Do the patterns we see from space truly reflect the conditions on the ground where disease transmission actually occurs?\nThis module addresses that question head-on. Validation is the process of rigorously assessing the accuracy of our models and maps against independent, real-world data, often called ground-truth data. Without this step, our maps are merely hypotheses. With it, they become evidence-based tools for public health action.\nBy the end of this notebook, you will be able to: 1. Understand the principle of ground-truthing and its importance in epidemiology."
  },
  {
    "objectID": "modules/module4-validation.html#setup-loading-packages",
    "href": "modules/module4-validation.html#setup-loading-packages",
    "title": "Module 4: Validating Remote Sensed Data with Ground-Truth Observations",
    "section": "4.1 Setup: Loading Packages",
    "text": "4.1 Setup: Loading Packages\nWe’ll use our standard packages, and add two new ones specifically for statistical validation: caret and pROC.\n\n\nShow/Hide Code\n# install.packages(\"caret\")\n# install.packages(\"pROC\")\n# install.packages(\"spDataLarge\") # In case you don't have it from Module 3\n\n\nNow, let’s load our required libraries.\n\n\nShow/Hide Code\n# Core Packages\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(dplyr)\n\n# Data Package with a sample satellite image\nlibrary(spDataLarge)\n\n# Validation Packages\nlibrary(caret)\nlibrary(pROC)"
  },
  {
    "objectID": "modules/module4-validation.html#linking-field-data-with-remote-sensing-products",
    "href": "modules/module4-validation.html#linking-field-data-with-remote-sensing-products",
    "title": "Module 4: Validating Remote Sensed Data with Ground-Truth Observations",
    "section": "4.2 Linking Field Data with Remote Sensing Products",
    "text": "4.2 Linking Field Data with Remote Sensing Products\nThe first step in any validation exercise is to link your ground-truth data to your remote sensing product. This process is called extraction.\n\nWorked Example: Extracting NDVI Values\nLet’s imagine we conducted a field survey and have the GPS locations of 100 random survey points. We want to find the NDVI value at each of these locations.\n\n\nShow/Hide Code\n# 1. Load the landsat raster and calculate NDVI using positional indices\nlandsat_path &lt;- system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nlandsat &lt;- rast(landsat_path)\nlandsat_ndvi &lt;- (landsat[[4]] - landsat[[3]]) / (landsat[[4]] + landsat[[3]]) # Using positional indices\n\n# 2. Simulate our ground-truth survey points\nset.seed(1984) # for reproducibility\ngt_points &lt;- spatSample(landsat_ndvi, size = 100, \"random\", as.points = TRUE)\ngt_points_sf &lt;- st_as_sf(gt_points) %&gt;% # Convert to sf object\n    select(-1) %&gt;% # remove the dummy raster value column\n    mutate(point_id = 1:100) # Give each point a unique ID\n\n# 3. Perform the extraction\nextracted_values &lt;- terra::extract(landsat_ndvi, gt_points_sf)\n\n# 4. Inspect the result\nhead(extracted_values)\n\n\n  ID  landsat_4\n1  1 0.24296411\n2  2 0.09798281\n3  3 0.18418055\n4  4 0.18159787\n5  5 0.11956828\n6  6 0.19367867\n\n\nShow/Hide Code\n# 5. Join the extracted values back to our original points data\ngt_points_with_ndvi &lt;- cbind(gt_points_sf, extracted_values)\nhead(gt_points_with_ndvi)\n\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 304710 ymin: 4113990 xmax: 335040 ymax: 4141110\nProjected CRS: WGS 84 / UTM zone 12N\n  point_id ID  landsat_4               geometry\n1        1  1 0.24296411 POINT (304710 4141110)\n2        2  2 0.09798281 POINT (305790 4120380)\n3        3  3 0.18418055 POINT (325800 4124700)\n4        4  4 0.18159787 POINT (312210 4130640)\n5        5  5 0.11956828 POINT (310500 4122330)\n6        6  6 0.19367867 POINT (335040 4113990)"
  },
  {
    "objectID": "modules/module4-validation.html#accuracy-assessment-for-classified-maps",
    "href": "modules/module4-validation.html#accuracy-assessment-for-classified-maps",
    "title": "Module 4: Validating Remote Sensed Data with Ground-Truth Observations",
    "section": "4.3 Accuracy Assessment for Classified Maps",
    "text": "4.3 Accuracy Assessment for Classified Maps\nTo assess the accuracy of a classified map (e.g., “Suitable Habitat” vs. “Unsuitable Habitat”), we use a Confusion Matrix.\n\n\n\n\nActual: Positive\nActual: Negative\n\n\n\n\nPred: Positive\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nPred: Negative\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\n\nWorked Example: Assessing a Habitat Suitability Map\nSince we don’t have a real classified map, we will simulate one to understand the methodology.\n\n\nShow/Hide Code\n# Step 1: Create a more clustered \"true\" habitat map\nset.seed(101)\n# Method 1: Using spatial autocorrelation with focal statistics\n# Start with random values\ntrue_habitat &lt;- rast(nrows=50, ncols=50, vals=runif(2500))\n\n# Apply multiple rounds of smoothing to create clusters\nfor(i in 1:3) {\n  true_habitat &lt;- focal(true_habitat, w=matrix(1,3,3), fun=mean, na.rm=TRUE)\n}\n\n# Convert to binary based on threshold - KEY FIX: convert to numeric\ntrue_habitat &lt;- true_habitat &gt; quantile(values(true_habitat), 0.6)\nvalues(true_habitat) &lt;- as.numeric(values(true_habitat))\nnames(true_habitat) &lt;- \"actual\"\n\n# Step 2: Create predicted map with ~20% error\npredicted_habitat &lt;- true_habitat\nn_errors &lt;- round(ncell(predicted_habitat) * 0.20)\nerror_cells &lt;- sample(1:ncell(predicted_habitat), n_errors)\npredicted_habitat[error_cells] &lt;- 1 - predicted_habitat[error_cells]\nnames(predicted_habitat) &lt;- \"predicted\"\n\n# Plot results\nplot(c(true_habitat, predicted_habitat), main=c(\"Actual Habitat (Clustered)\", \"Predicted Habitat\"))\n\n\n\n\n\n\n\n\n\nShow/Hide Code\n# Step 3: Generate validation points and extract values\nvalidation_points &lt;- spatSample(true_habitat, size = 250, \"random\", xy=TRUE)\nvalidation_points$predicted &lt;- terra::extract(predicted_habitat, validation_points[,c(\"x\",\"y\")])$predicted\n\n# Step 4: Prepare data for the confusion matrix\nvalidation_data &lt;- as.data.frame(validation_points)\n\n# Convert to factors - make sure we have 0/1 values first\nvalidation_data$predicted &lt;- factor(validation_data$predicted, levels = c(0, 1), labels = c(\"Unsuitable\", \"Suitable\"))\nvalidation_data$actual    &lt;- factor(validation_data$actual,    levels = c(0, 1), labels = c(\"Unsuitable\", \"Suitable\"))\n\n\nNow we have our validation_data with “predicted” and “actual” columns. Let’s create the confusion matrix.\n\n\nShow/Hide Code\n# Step 5: Create and interpret the confusion matrix using caret\nconf_matrix &lt;- caret::confusionMatrix(data = validation_data$predicted, \n                                     reference = validation_data$actual,\n                                     positive = \"Suitable\")\n\n# Print the full output\nprint(conf_matrix)\n\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   Unsuitable Suitable\n  Unsuitable        118       22\n  Suitable           26       84\n                                          \n               Accuracy : 0.808           \n                 95% CI : (0.7536, 0.8549)\n    No Information Rate : 0.576           \n    P-Value [Acc &gt; NIR] : 6.238e-15       \n                                          \n                  Kappa : 0.6089          \n                                          \n Mcnemar's Test P-Value : 0.665           \n                                          \n            Sensitivity : 0.7925          \n            Specificity : 0.8194          \n         Pos Pred Value : 0.7636          \n         Neg Pred Value : 0.8429          \n             Prevalence : 0.4240          \n         Detection Rate : 0.3360          \n   Detection Prevalence : 0.4400          \n      Balanced Accuracy : 0.8059          \n                                          \n       'Positive' Class : Suitable"
  },
  {
    "objectID": "modules/module4-validation.html#evaluating-probabilistic-models-with-roc-curves",
    "href": "modules/module4-validation.html#evaluating-probabilistic-models-with-roc-curves",
    "title": "Module 4: Validating Remote Sensed Data with Ground-Truth Observations",
    "section": "4.4 Evaluating Probabilistic Models with ROC Curves",
    "text": "4.4 Evaluating Probabilistic Models with ROC Curves\nTo evaluate models that predict a continuous probability, we use a Receiver Operating Characteristic (ROC) curve. The key summary metric is the Area Under the Curve (AUC), where 1.0 is a perfect model and 0.5 is no better than random chance.\n\nWorked Example: ROC Curve Analysis\nLet’s simulate some data for a disease presence/absence study.\n\n\nShow/Hide Code\n# Step 1: Simulate data\nset.seed(42)\nn_samples &lt;- 200\nroc_data &lt;- data.frame(\n  # The true presence (1) or absence (0) of the disease\n  ground_truth = rbinom(n_samples, 1, 0.5)\n)\n\n# Simulate model probabilities. A good model will give higher scores to the 'presence' cases.\nroc_data$model_prob &lt;- ifelse(roc_data$ground_truth == 1, \n                              rnorm(n_samples, mean=0.7, sd=0.2), \n                              rnorm(n_samples, mean=0.3, sd=0.2))\n# Ensure probabilities are between 0 and 1\nroc_data$model_prob[roc_data$model_prob &gt; 1] &lt;- 1\nroc_data$model_prob[roc_data$model_prob &lt; 0] &lt;- 0\n\n# Step 2: Generate the ROC curve object using the pROC package\nroc_object &lt;- roc(ground_truth ~ model_prob, data = roc_data)\n\n# Step 3: Calculate the AUC\nauc_value &lt;- auc(roc_object)\nprint(paste(\"Area Under Curve (AUC):\", round(auc_value, 3)))\n\n\n[1] \"Area Under Curve (AUC): 0.922\"\n\n\nShow/Hide Code\n# Step 4: Plot the ROC curve\nplot(roc_object, main = \"ROC Curve\", print.auc = TRUE)\n\n\n\n\n\n\n\n\n\nOur simulated model has an AUC of 0.92, indicating it has excellent discriminatory power."
  },
  {
    "objectID": "modules/module2-advanced.html",
    "href": "modules/module2-advanced.html",
    "title": "Module 2: Advanced Spatial Data Handling and Operations",
    "section": "",
    "text": "In Module 1, we learned the essentials: loading, inspecting, and creating basic visualizations of vector and raster data. Now, we move beyond displaying data to truly analyzing it by exploring the relationships between different spatial datasets.\nThis module covers the core data wrangling operations that are the workhorses of any real-world geospatial analysis.\nBy the end of this notebook, you will be able to:"
  },
  {
    "objectID": "modules/module2-advanced.html#setup-loading-packages-and-installation",
    "href": "modules/module2-advanced.html#setup-loading-packages-and-installation",
    "title": "Module 2: Advanced Spatial Data Handling and Operations",
    "section": "2.1 Setup: Loading Packages and Installation",
    "text": "2.1 Setup: Loading Packages and Installation\nAs before, we begin by loading the packages we need. This module introduces rayshader, a powerful package for 3D visualization.\n\n\nShow/Hide Code\n# Run these lines in your R console if you haven't installed rayshader yet\n# install.packages(\"devtools\")\ndevtools::install_github(\"tylermorganwall/rayshader\")\n\n\n\n\nShow/Hide Code\n# Core Packages\nlibrary(sf)\nlibrary(terra)\nlibrary(tmap)\nlibrary(dplyr)\nlibrary(rayshader) # For 3D visualization\nlibrary(ggplot2)  # Used by some rayshader examples\n\n# Data Packages\nlibrary(spData)\nlibrary(maps)\nlibrary(spDataLarge)\nlibrary(geodata)"
  },
  {
    "objectID": "modules/module2-advanced.html#advanced-vector-data-techniques-with-sf",
    "href": "modules/module2-advanced.html#advanced-vector-data-techniques-with-sf",
    "title": "Module 2: Advanced Spatial Data Handling and Operations",
    "section": "2.2 Advanced Vector Data Techniques with sf",
    "text": "2.2 Advanced Vector Data Techniques with sf\n\nSpatial Joins\nA spatial join is similar to a regular dplyr::left_join(), but instead of joining two tables based on a common ID column, it joins them based on their spatial relationship. This is an incredibly powerful tool for enriching your data.\nWorked Example: Assigning Points to Polygons\nImagine we have a dataset of GPS coordinates for several disease outbreak locations, and we want to know which country each outbreak is in.\n\n\nShow/Hide Code\n# First, let's get our polygon layer from the world dataset\ndata(world) # Make sure world data is loaded\nsa_countries &lt;- world %&gt;%\n  filter(continent == \"South America\")\n\n# Next, let's create a dummy sf object of outbreak points.\nset.seed(2024) # for reproducibility\noutbreak_points &lt;- st_sample(sa_countries, size = 15) %&gt;%\n  st_as_sf() %&gt;% # Convert the sfc_POINT object to an sf data frame\n  mutate(outbreak_id = 1:15) # Add an ID for each outbreak\n\n# Perform the spatial join\npoints_with_country_data &lt;- st_join(outbreak_points, sa_countries)\n\n# Let's look at the result\nprint(points_with_country_data)\n\n\nSimple feature collection with 15 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -68.42411 ymin: -30.40425 xmax: -39.3102 ymax: 1.299283\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   outbreak_id iso_a2 name_long     continent region_un     subregion\n1            1     BO   Bolivia South America  Americas South America\n2            2     BR    Brazil South America  Americas South America\n3            3     BR    Brazil South America  Americas South America\n4            4     BR    Brazil South America  Americas South America\n5            5     BR    Brazil South America  Americas South America\n6            6     AR Argentina South America  Americas South America\n7            7     BR    Brazil South America  Americas South America\n8            8     BR    Brazil South America  Americas South America\n9            9     BR    Brazil South America  Americas South America\n10          10     BR    Brazil South America  Americas South America\n                type area_km2       pop lifeExp gdpPercap\n1  Sovereign country  1085270  10562159  68.357  6324.827\n2  Sovereign country  8508557 204213133  75.042 15374.262\n3  Sovereign country  8508557 204213133  75.042 15374.262\n4  Sovereign country  8508557 204213133  75.042 15374.262\n5  Sovereign country  8508557 204213133  75.042 15374.262\n6  Sovereign country  2784469  42981515  76.252 18797.548\n7  Sovereign country  8508557 204213133  75.042 15374.262\n8  Sovereign country  8508557 204213133  75.042 15374.262\n9  Sovereign country  8508557 204213133  75.042 15374.262\n10 Sovereign country  8508557 204213133  75.042 15374.262\n                             x\n1  POINT (-66.43254 -21.48096)\n2  POINT (-48.66798 -7.045351)\n3   POINT (-62.00516 -6.69752)\n4   POINT (-39.3102 -8.718425)\n5  POINT (-57.33359 -10.13765)\n6  POINT (-60.42989 -24.66893)\n7  POINT (-67.62014 -8.297282)\n8  POINT (-49.64755 -3.098395)\n9  POINT (-41.73221 -20.72512)\n10  POINT (-44.18993 -18.4131)\n\n\nShow/Hide Code\n# Let's map this to see the result\ntmap_mode(\"plot\")\ntm_shape(sa_countries) + tm_polygons() + tm_borders() +\n  tm_shape(points_with_country_data) + \n  tm_dots(col = \"name_long\", palette = \"viridis\", size=0.7, title=\"Country\") +\n  tm_layout(main.title = \"Outbreaks Assigned to Countries\")\n\n\n\n\n\n\n\n\n\n\n\nBuffering\nBuffering creates a new polygon around a spatial feature at a specified distance. This is useful for modeling zones of influence or potential exposure.\nImportant: Buffering requires a projected CRS for distances to be meaningful (e.g., in meters).\nWorked Example: Buffering Health Facilities\nLet’s use the zion dataset from spData. We can pretend the visitor centers are health clinics.\n\n\nShow/Hide Code\n# Load Zion National Park data\nzion_gpkg_path &lt;- system.file(\"vector/zion_points.gpkg\", package = \"spDataLarge\")\nzion_boundary_gpkg_path &lt;- system.file(\"vector/zion.gpkg\", package = \"spDataLarge\")\n\nzion_boundary &lt;- sf::read_sf(zion_boundary_gpkg_path)\nzion_points &lt;- sf::read_sf(zion_gpkg_path)\n\n# First, CHECK THE CRS! The units are meters. Perfect for buffering!\nst_crs(zion_points)\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        MEMBER[\"World Geodetic System 1984 (G2296)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nShow/Hide Code\n# Let's create a 1.5 kilometer (1500 meter) buffer around these points\nvisitor_center_buffers &lt;- st_buffer(zion_points, dist = 1500)\n\n# Let's map the park boundary, the points, and their buffers\ntm_shape(zion_boundary) + tm_polygons(col=\"lightgreen\", alpha=0.5) + tm_borders() +\n  tm_shape(zion_points) + tm_dots(col=\"red\", size=0.5) +\n  tm_shape(visitor_center_buffers) + tm_polygons(col=\"blue\", alpha=0.4, border.col = \"darkblue\") +\n  tm_layout(main.title = \"1.5km Buffers around Visitor Centers\")\n\n\n\n\n\n\n\n\n\n\n\nGeometric Operations\nThe sf package allows for powerful geometric operations. The two most common are st_intersection() (finding common area) and st_union() (dissolving boundaries).\nWorked Example (Union): Creating a Single Regional Polygon\n\n\nShow/Hide Code\n# Get the western states from Module 1\ndata(\"us_states\")\nwestern_states &lt;- us_states %&gt;% filter(REGION == \"West\")\n\n# Dissolve all the internal boundaries into a single feature\nwest_region_boundary &lt;- st_union(western_states)\n\n# Plot the original states and the single dissolved boundary\nplot1 &lt;- tm_shape(western_states) + tm_polygons(\"NAME\", legend.show = FALSE) + tm_layout(title=\"Original States\")\nplot2 &lt;- tm_shape(west_region_boundary) + tm_polygons(col=\"lightblue\") + tm_layout(title=\"Union of States\")\ntmap_arrange(plot1, plot2)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1: Advanced Vector Operations\nLet’s practice by finding major cities in Brazil and analyzing their proximity.\n\nTask 1Try it yourself!Solution\n\n\nLoad the world.cities dataset from spData. Spatially join it with the world dataset to get the country name for each city. Then, filter to create a new sf object called brazil_cities that contains only cities in Brazil.\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Re-create all necessary objects from previous tasks\ndata(\"world.cities\", package = \"maps\")\ndata(\"world\")\nworld_cities_sf &lt;- st_as_sf(world.cities, coords = c(\"long\", \"lat\"), crs = 4326)\ncities_in_countries &lt;- st_join(world_cities_sf, world)\n\nbrazil_cities &lt;- cities_in_countries %&gt;% filter(name_long == \"Brazil\")\n\nhead(brazil_cities)\n\n\nSimple feature collection with 6 features and 14 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -48.89 ymin: -7.95 xmax: -34.88 ymax: -1.72\nGeodetic CRS:  WGS 84\n          name country.etc pop.x capital iso_a2 name_long     continent\n1   Abaetetuba      Brazil 79660       0     BR    Brazil South America\n2 Abreu e Lima      Brazil 82765       0     BR    Brazil South America\n3   Acailandia      Brazil 93252       0     BR    Brazil South America\n4       Acarau      Brazil 29169       0     BR    Brazil South America\n5     Acopiara      Brazil 24942       0     BR    Brazil South America\n6          Acu      Brazil 36421       0     BR    Brazil South America\n  region_un     subregion              type area_km2     pop.y lifeExp\n1  Americas South America Sovereign country  8508557 204213133  75.042\n2  Americas South America Sovereign country  8508557 204213133  75.042\n3  Americas South America Sovereign country  8508557 204213133  75.042\n4  Americas South America Sovereign country  8508557 204213133  75.042\n5  Americas South America Sovereign country  8508557 204213133  75.042\n6  Americas South America Sovereign country  8508557 204213133  75.042\n  gdpPercap             geometry\n1  15374.26 POINT (-48.89 -1.72)\n2  15374.26 POINT (-34.88 -7.95)\n3  15374.26 POINT (-47.54 -5.06)\n4  15374.26 POINT (-40.12 -2.89)\n5  15374.26  POINT (-39.46 -6.1)\n6  15374.26 POINT (-36.91 -5.58)\n\n\n\n\n\n\nTask 2Try it yourself!Solution\n\n\nThe CRS of brazil_cities is WGS84 (geographic). To create meaningful buffers, transform it to a projected CRS suitable for Brazil. A good one is SIRGAS 2000 / Brazil Polyconic (EPSG:5880). Then, create a 50km (50,000 meter) buffer around each city.\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Re-create all necessary objects from previous tasks\ndata(\"world.cities\", package = \"maps\")\ndata(\"world\")\nworld_cities_sf &lt;- st_as_sf(world.cities, coords = c(\"long\", \"lat\"), crs = 4326)\ncities_in_countries &lt;- st_join(world_cities_sf, world)\nbrazil_cities &lt;- cities_in_countries %&gt;% filter(name_long == \"Brazil\")\n\n\n# Task\nbrazil_cities_proj &lt;- st_transform(brazil_cities, \"EPSG:5880\")\ncity_buffers_proj &lt;- st_buffer(brazil_cities_proj, dist = 50000)\n\nhead(city_buffers_proj)\n\n\nSimple feature collection with 6 features and 14 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 5518587 ymin: 9022237 xmax: 7157359 ymax: 9859051\nProjected CRS: SIRGAS 2000 / Brazil Polyconic\n          name country.etc pop.x capital iso_a2 name_long     continent\n1   Abaetetuba      Brazil 79660       0     BR    Brazil South America\n2 Abreu e Lima      Brazil 82765       0     BR    Brazil South America\n3   Acailandia      Brazil 93252       0     BR    Brazil South America\n4       Acarau      Brazil 29169       0     BR    Brazil South America\n5     Acopiara      Brazil 24942       0     BR    Brazil South America\n6          Acu      Brazil 36421       0     BR    Brazil South America\n  region_un     subregion              type area_km2     pop.y lifeExp\n1  Americas South America Sovereign country  8508557 204213133  75.042\n2  Americas South America Sovereign country  8508557 204213133  75.042\n3  Americas South America Sovereign country  8508557 204213133  75.042\n4  Americas South America Sovereign country  8508557 204213133  75.042\n5  Americas South America Sovereign country  8508557 204213133  75.042\n6  Americas South America Sovereign country  8508557 204213133  75.042\n  gdpPercap                       geometry\n1  15374.26 POLYGON ((5618587 9809051, ...\n2  15374.26 POLYGON ((7157359 9072237, ...\n3  15374.26 POLYGON ((5766328 9436918, ...\n4  15374.26 POLYGON ((6593124 9671014, ...\n5  15374.26 POLYGON ((6659287 9303771, ...\n6  15374.26 POLYGON ((6943230 9355519, ...\n\n\n\n\n\n\nTask 3Try it yourself!Solution\n\n\nThe buffers might extend beyond Brazil’s land border. Get the polygon for Brazil from the world dataset, transform it to the same projected CRS (EPSG:5880), and then find the intersection of your buffers and the country’s polygon.\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Re-create all necessary objects from previous tasks\ndata(\"world.cities\")\ndata(\"world\")\nworld_cities_sf &lt;- st_as_sf(world.cities, coords = c(\"long\", \"lat\"), crs = 4326)\ncities_in_countries &lt;- st_join(world_cities_sf, world)\nbrazil_cities &lt;- cities_in_countries %&gt;% filter(name_long == \"Brazil\")\nbrazil_cities_proj &lt;- st_transform(brazil_cities, \"EPSG:5880\")\ncity_buffers_proj &lt;- st_buffer(brazil_cities_proj, dist = 50000)\n\n# Task\nbrazil_poly_proj &lt;- world %&gt;% \n  filter(name_long == \"Brazil\") %&gt;%\n  st_transform(\"EPSG:5880\")\ncity_buffers_intersect &lt;- st_intersection(city_buffers_proj, brazil_poly_proj)\nhead(city_buffers_intersect)\n\n\nSimple feature collection with 6 features and 24 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 5518587 ymin: 9023997 xmax: 7120924 ymax: 9859051\nProjected CRS: SIRGAS 2000 / Brazil Polyconic\n          name country.etc pop.x capital iso_a2 name_long     continent\n1   Abaetetuba      Brazil 79660       0     BR    Brazil South America\n2 Abreu e Lima      Brazil 82765       0     BR    Brazil South America\n3   Acailandia      Brazil 93252       0     BR    Brazil South America\n4       Acarau      Brazil 29169       0     BR    Brazil South America\n5     Acopiara      Brazil 24942       0     BR    Brazil South America\n6          Acu      Brazil 36421       0     BR    Brazil South America\n  region_un     subregion              type area_km2     pop.y lifeExp\n1  Americas South America Sovereign country  8508557 204213133  75.042\n2  Americas South America Sovereign country  8508557 204213133  75.042\n3  Americas South America Sovereign country  8508557 204213133  75.042\n4  Americas South America Sovereign country  8508557 204213133  75.042\n5  Americas South America Sovereign country  8508557 204213133  75.042\n6  Americas South America Sovereign country  8508557 204213133  75.042\n  gdpPercap iso_a2.1 name_long.1   continent.1 region_un.1   subregion.1\n1  15374.26       BR      Brazil South America    Americas South America\n2  15374.26       BR      Brazil South America    Americas South America\n3  15374.26       BR      Brazil South America    Americas South America\n4  15374.26       BR      Brazil South America    Americas South America\n5  15374.26       BR      Brazil South America    Americas South America\n6  15374.26       BR      Brazil South America    Americas South America\n             type.1 area_km2.1       pop lifeExp.1 gdpPercap.1\n1 Sovereign country    8508557 204213133    75.042    15374.26\n2 Sovereign country    8508557 204213133    75.042    15374.26\n3 Sovereign country    8508557 204213133    75.042    15374.26\n4 Sovereign country    8508557 204213133    75.042    15374.26\n5 Sovereign country    8508557 204213133    75.042    15374.26\n6 Sovereign country    8508557 204213133    75.042    15374.26\n                        geometry\n1 POLYGON ((5618519 9806434, ...\n2 POLYGON ((7091908 9024684, ...\n3 POLYGON ((5766260 9434301, ...\n4 POLYGON ((6589803 9653095, ...\n5 POLYGON ((6659218 9301155, ...\n6 POLYGON ((6943161 9352903, ...\n\n\n\n\n\n\nTask 4Try it yourself!Solution\n\n\nCreate a map showing the final intersected buffers on top of the Brazil polygon.\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Re-create all necessary objects\ndata(\"world.cities\")\ndata(\"world\")\nworld_cities_sf &lt;- st_as_sf(world.cities, coords = c(\"long\", \"lat\"), crs = 4326)\ncities_in_countries &lt;- st_join(world_cities_sf, world)\nbrazil_cities &lt;- cities_in_countries %&gt;% filter(name_long == \"Brazil\")\nbrazil_cities_proj &lt;- st_transform(brazil_cities, \"EPSG:5880\")\ncity_buffers_proj &lt;- st_buffer(brazil_cities_proj, dist = 50000)\nbrazil_poly_proj &lt;- world %&gt;% \n  filter(name_long == \"Brazil\") %&gt;%\n  st_transform(\"EPSG:5880\")\ncity_buffers_intersect &lt;- st_intersection(city_buffers_proj, brazil_poly_proj)\n\n# Create the map\ntm_shape(brazil_poly_proj) + tm_polygons() +\n  tm_shape(city_buffers_intersect) + tm_polygons(col=\"red\", alpha=0.5) +\n  tm_layout(main.title = \"50km Buffer Zones around Major Brazilian Cities\")"
  },
  {
    "objectID": "modules/module2-advanced.html#advanced-raster-data-techniques-with-terra",
    "href": "modules/module2-advanced.html#advanced-raster-data-techniques-with-terra",
    "title": "Module 2: Advanced Spatial Data Handling and Operations",
    "section": "2.3 Advanced Raster Data Techniques with terra",
    "text": "2.3 Advanced Raster Data Techniques with terra\n\nRaster Calculations (Map Algebra)\nMap algebra is the process of performing calculations on one or more raster layers. With terra, this is as simple as using standard R arithmetic operators.\nWorked Example: Calculating Temperature Range\n\n\nShow/Hide Code\n# Load the data and calculate the annual mean in Celsius\ntemp_path &lt;- tempdir()\nglobal_tmax &lt;- geodata::worldclim_global(var = \"tmax\", res = 10, path = temp_path) / 10\n\n# Access individual layers (months) using [[...]]\njan_tmax &lt;- global_tmax[[1]]\njul_tmax &lt;- global_tmax[[7]]\n\n# Perform the calculation\nannual_range &lt;- jan_tmax - jul_tmax\n\n# Rename the layer for the plot legend\nnames(annual_range) &lt;- \"Temp Range (°C)\"\n\n# Plot the result\ntm_shape(annual_range) + tm_raster(palette=\"RdBu\", style=\"cont\") +\n  tm_layout(main.title = \"Annual Max Temp Range (Jan - July)\")\n\n\n\n\n\n\n\n\n\n\n\nZonal Statistics\nThis is arguably one of the most important techniques for environmental epidemiology. It involves summarizing raster values within zones defined by vector polygons (e.g., calculating mean NDVI per administrative district).\nWorked Example: Mean Elevation per Country in South America\n\n\nShow/Hide Code\n# 1. Get the polygon data (zones)\nsa_countries &lt;- world %&gt;%\n  filter(continent == \"South America\")\n\n# 2. Get the raster data (values)\n# We will download a coarse global elevation raster\ntemp_path &lt;- tempdir() # Ensure temp_path is defined\nelevation &lt;- geodata::worldclim_global(var = \"elev\", res = 10, path = temp_path)\n\n# 3. Perform the zonal extraction\nmean_elev &lt;- terra::extract(elevation, sa_countries, fun = \"mean\", ID = FALSE)\n\n# 4. Combine the results with the polygon data\nsa_countries_with_elev &lt;- cbind(sa_countries, mean_elev)\n\n# 5. Map the result\ntm_shape(sa_countries_with_elev) +\n  tm_polygons(col = \"wc2.1_10m_elev\", \n              title = \"Mean Elevation (m)\", \n              palette = \"YlOrBr\") +\n  tm_layout(main.title = \"Mean Elevation by Country in South America\")"
  },
  {
    "objectID": "modules/module2-advanced.html#d-visualization-with-rayshader",
    "href": "modules/module2-advanced.html#d-visualization-with-rayshader",
    "title": "Module 2: Advanced Spatial Data Handling and Operations",
    "section": "2.4 3D Visualization with rayshader",
    "text": "2.4 3D Visualization with rayshader\nWhile tmap and ggplot2 are excellent for 2D maps, the rayshader package allows us to create stunning 2D and 3D data visualizations, adding a new dimension to our analysis. It uses elevation data in a standard R matrix and a combination of raytracing and hillshading algorithms to generate beautiful maps.\n\nWorked Example: Creating a 3D Map of Monterey Bay\nThe core rayshader workflow involves taking an elevation matrix and layering different shade components (color, shadow, ambient light) before plotting it in 2D or 3D. We will use the montereybay dataset, which is built into rayshader.\n\n\nShow/Hide Code\n# 1. Get Data: The `montereybay` dataset is already a matrix.\nelmat &lt;- montereybay\n\n# 2. Create a 2D map by layering shades\n# Start with a texture layer based on elevation\nmap_base &lt;- elmat %&gt;%\n  sphere_shade(texture = \"imhof1\")\n\n# Add lambertian and ambient shadow layers\nmap_with_shadows &lt;- map_base %&gt;%\n  add_shadow(ray_shade(elmat, zscale = 50, lambert = FALSE), 0.5) %&gt;%\n  add_shadow(ambient_shade(elmat, zscale = 50), 0)\n\n# Plot the final 2D map\nplot_map(map_with_shadows)\n\n# 3. Create a 3D Map with a water layer\n# In plot_3d, we can add water directly.\n# The waterdepth is 0, since that's sea level in this dataset.\nmap_with_shadows %&gt;%\n  plot_3d(elmat, zscale = 50, fov = 0, theta = -45, phi = 45, \n          windowsize = c(1000, 800), zoom = 0.75,\n          water = TRUE, waterdepth = 0, wateralpha = 0.5, watercolor = \"lightblue\",\n          waterlinecolor = \"white\", waterlinealpha = 0.5)\n\n# Use render_snapshot() to capture the current 3D view\nrender_snapshot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: 3D Visualization with rayshader\nLet’s apply these 3D visualization skills to the elevation data for South America we used in the zonal statistics example.\n\nTask 1Try it yourself!Solution\n\n\nCrop the global elevation raster (elevation) to the extent of South America (sa_countries). Then, convert the resulting SpatRaster object into a matrix using raster_to_matrix().\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Re-create necessary objects\ndata(\"world\")\nsa_countries &lt;- world %&gt;% filter(continent == \"South America\")\ntemp_path &lt;- tempdir()\nelevation &lt;- geodata::worldclim_global(var = \"elev\", res = 10, path = temp_path)\n\n# Task\nsa_elevation_raster &lt;- crop(elevation, sa_countries)\nsa_elevation_matrix &lt;- raster_to_matrix(sa_elevation_raster)\n\n\n\n\n\n\nTask 2Try it yourself!Solution\n\n\nUsing the South America elevation matrix, create a 2D map. 1. Start with sphere_shade(). Try the imhof4 texture. 2. Add a water layer using add_water() and detect_water(). 3. Add a shadow layer using ray_shade(). 4. Plot the result with plot_map().\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Re-create matrix from previous task\ndata(\"world\")\nsa_countries &lt;- world %&gt;% filter(continent == \"South America\")\ntemp_path &lt;- tempdir()\nelevation &lt;- geodata::worldclim_global(var = \"elev\", res = 10, path = temp_path)\nsa_elevation_raster &lt;- crop(elevation, sa_countries)\nsa_elevation_matrix &lt;- raster_to_matrix(sa_elevation_raster)\n\n# Task\nsa_elevation_matrix %&gt;%\n  sphere_shade(texture = \"imhof4\") %&gt;%\n  add_water(detect_water(sa_elevation_matrix), color = \"imhof4\") %&gt;%\n  add_shadow(ray_shade(sa_elevation_matrix, zscale = 3), 0.5) %&gt;%\n  plot_map()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 3Try it yourself!Solution\n\n\nTake the 2D map you just created and render it in 3D using plot_3d(). Experiment with the theta and phi camera arguments to get a nice view of the Andes mountains. Use render_snapshot() to save the view.\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Re-create objects from previous task\ndata(\"world\")\nsa_countries &lt;- world %&gt;% filter(continent == \"South America\")\ntemp_path &lt;- tempdir()\nelevation &lt;- geodata::worldclim_global(var = \"elev\", res = 10, path = temp_path)\nsa_elevation_raster &lt;- crop(elevation, sa_countries)\n# Keep the original matrix with NAs for water detection\nsa_elevation_matrix_raw &lt;- raster_to_matrix(sa_elevation_raster)\n\nsa_elevation_matrix_filled &lt;- sa_elevation_matrix_raw\nmin_elev &lt;- min(sa_elevation_matrix_filled, na.rm = TRUE)\nsa_elevation_matrix_filled[is.na(sa_elevation_matrix_filled)] &lt;- min_elev\n\n# Create the map object using the 'filled' matrix for shading\n# but the 'raw' matrix for detecting water.\nsa_map &lt;- sa_elevation_matrix_filled %&gt;%\n  sphere_shade(texture = \"imhof4\") %&gt;%\n  add_water(detect_water(sa_elevation_matrix_raw), color = \"imhof4\") %&gt;%\n  add_shadow(ray_shade(sa_elevation_matrix_filled, zscale = 3), 0.5)\n\n# Task: Plot in 3D using the 'filled' matrix\nsa_map %&gt;%\n  plot_3d(sa_elevation_matrix_filled, zscale = 50, fov = 0, theta = 45, phi = 30, \n          windowsize = c(1000, 800), zoom = 0.7)\n\nrender_snapshot()"
  },
  {
    "objectID": "modules/module1-essentials.html",
    "href": "modules/module1-essentials.html",
    "title": "Module 1: Mastering R for Spatial Data",
    "section": "",
    "text": "The goal of this module is to build a rock-solid foundation in handling, manipulating, and visualizing spatial data within the R environment. We will focus on the “tidyverse” approach to spatial analysis, which treats spatial data as a special kind of data frame, making many operations intuitive and powerful.\nBy the end of this notebook, you will be able to:"
  },
  {
    "objectID": "modules/module1-essentials.html#setup-installing-and-loading-core-packages",
    "href": "modules/module1-essentials.html#setup-installing-and-loading-core-packages",
    "title": "Module 1: Mastering R for Spatial Data",
    "section": "1.1 Setup: Installing and Loading Core Packages",
    "text": "1.1 Setup: Installing and Loading Core Packages\nFor our work, we will rely on a core set of packages that form the backbone of modern spatial analysis in R.\nIf you have not installed these packages before, you must do so first. You can run the install.packages() function in your R console for each one. You only need to do this once.\n\n\nShow/Hide Code\n# Run these lines in your R console if you haven't installed the packages yet\n# install.packages(\"sf\")\n# install.packages(\"terra\")\n# install.packages(\"tmap\")\n# install.packages(\"ggplot2\") # The premier package for data visualization\n# install.packages(\"dplyr\")\n# install.packages(\"spData\")\n# install.packages(\"geodata\")\n# install.packages(\"spDataLarge\", repos = \"https://nowosad.github.io/drat/\", type = \"source\")\n\n\nOnce installed, we need to load the packages into our current R session using the library() function. We do this at the start of every script.\n\n\nShow/Hide Code\n# --- Core Packages ---\nlibrary(sf)         # Handles vector data (Simple Features). The modern standard.\nlibrary(terra)      # Handles raster data. The modern, high-performance successor to the 'raster' package.\nlibrary(tmap)       # Used for creating beautiful thematic maps (like ggplot2 for maps).\nlibrary(ggplot2)    # A powerful and versatile grammar of graphics for visualization.\nlibrary(dplyr)      # A grammar of data manipulation, works seamlessly with 'sf'.\n\n# --- Data Packages ---\nlibrary(spData)     # Contains example spatial datasets for practice.\nlibrary(geodata)    # For downloading common global spatial datasets like country boundaries or climate data."
  },
  {
    "objectID": "modules/module1-essentials.html#vector-data-with-sf",
    "href": "modules/module1-essentials.html#vector-data-with-sf",
    "title": "Module 1: Mastering R for Spatial Data",
    "section": "1.2 Vector Data with sf",
    "text": "1.2 Vector Data with sf\n\nWhat is Vector Data?\nVector data represents geographical features using discrete geometric objects: Points, Lines, and Polygons.\n\n\nThe sf Object\nThe sf package provides a framework for working with vector data in R. An sf object is fundamentally an R data frame with a special list-column that stores the geometry. This structure is powerful because it allows you to use dplyr verbs directly on your spatial data.\n\n\nWorked Example: Exploring World Countries\nLet’s load the world dataset from the spData package and see what an sf object looks like.\n\n\nShow/Hide Code\n# Load the 'world' dataset\ndata(world)\n\n# 1. Inspect the object's class\nclass(world)\n\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nShow/Hide Code\n# 2. Print the object\nprint(world)\n\n\nSimple feature collection with 177 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 11\n   iso_a2 name_long continent region_un subregion type  area_km2     pop lifeExp\n * &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 FJ     Fiji      Oceania   Oceania   Melanesia Sove…   1.93e4  8.86e5    70.0\n 2 TZ     Tanzania  Africa    Africa    Eastern … Sove…   9.33e5  5.22e7    64.2\n 3 EH     Western … Africa    Africa    Northern… Inde…   9.63e4 NA         NA  \n 4 CA     Canada    North Am… Americas  Northern… Sove…   1.00e7  3.55e7    82.0\n 5 US     United S… North Am… Americas  Northern… Coun…   9.51e6  3.19e8    78.8\n 6 KZ     Kazakhst… Asia      Asia      Central … Sove…   2.73e6  1.73e7    71.6\n 7 UZ     Uzbekist… Asia      Asia      Central … Sove…   4.61e5  3.08e7    71.0\n 8 PG     Papua Ne… Oceania   Oceania   Melanesia Sove…   4.65e5  7.76e6    65.2\n 9 ID     Indonesia Asia      Asia      South-Ea… Sove…   1.82e6  2.55e8    68.9\n10 AR     Argentina South Am… Americas  South Am… Sove…   2.78e6  4.30e7    76.3\n# ℹ 167 more rows\n# ℹ 2 more variables: gdpPercap &lt;dbl&gt;, geom &lt;MULTIPOLYGON [°]&gt;\n\n\nShow/Hide Code\n# 3. Manipulate data with dplyr\nworld_africa &lt;- world %&gt;%\n  filter(continent == \"Africa\") %&gt;%\n  select(name_long, pop, gdpPercap, lifeExp, geom)\n\n# Let's look at our new, smaller sf object\nprint(world_africa)\n\n\nSimple feature collection with 51 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -17.62504 ymin: -34.81917 xmax: 51.13387 ymax: 37.34999\nGeodetic CRS:  WGS 84\n# A tibble: 51 × 5\n   name_long                     pop gdpPercap lifeExp                      geom\n   &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 Tanzania                   5.22e7     2402.    64.2 (((33.90371 -0.95, 31.86…\n 2 Western Sahara            NA            NA     NA   (((-8.66559 27.65643, -8…\n 3 Democratic Republic of t…  7.37e7      785.    58.8 (((29.34 -4.499983, 29.2…\n 4 Somalia                    1.35e7       NA     55.5 (((41.58513 -1.68325, 41…\n 5 Kenya                      4.60e7     2753.    66.2 (((39.20222 -4.67677, 39…\n 6 Sudan                      3.77e7     4188.    64.0 (((23.88711 8.619775, 24…\n 7 Chad                       1.36e7     2077.    52.2 (((23.83766 19.58047, 19…\n 8 South Africa               5.45e7    12390.    61.0 (((16.34498 -28.57671, 1…\n 9 Lesotho                    2.15e6     2677.    53.3 (((28.97826 -28.9556, 28…\n10 Zimbabwe                   1.54e7     1925.    59.4 (((31.19141 -22.25151, 3…\n# ℹ 41 more rows\n\n\nShow/Hide Code\n# 4. Check the Coordinate Reference System (CRS)\nst_crs(world_africa)\n\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nShow/Hide Code\n# 5. Basic Plotting\nplot(st_geometry(world_africa), main = \"Map of Africa (Geometries Only)\")\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 1: Vector Data Manipulation\nNow it’s your turn. Use the us_states dataset, which is also included in the spData package.\n\nTask 1Try it yourself!Solution\n\n\nLoad the us_states data and inspect its Coordinate Reference System (CRS).\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\ndata(us_states)\nst_crs(us_states)\n\n\nCoordinate Reference System:\n  User input: EPSG:4269 \n  wkt:\nGEOGCS[\"NAD83\",\n    DATUM[\"North_American_Datum_1983\",\n        SPHEROID[\"GRS 1980\",6378137,298.257222101,\n            AUTHORITY[\"EPSG\",\"7019\"]],\n        TOWGS84[0,0,0,0,0,0,0],\n        AUTHORITY[\"EPSG\",\"6269\"]],\n    PRIMEM[\"Greenwich\",0,\n        AUTHORITY[\"EPSG\",\"8901\"]],\n    UNIT[\"degree\",0.0174532925199433,\n        AUTHORITY[\"EPSG\",\"9122\"]],\n    AUTHORITY[\"EPSG\",\"4269\"]]\n\n\n\n\n\n\nTask 2Your turn!Solution\n\n\nUse dplyr::filter() to create a new sf object containing only the states in the “West” region.\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\nwestern_states &lt;- us_states %&gt;%\n  filter(REGION == \"West\")\n\n# Optional: Print the names to check your work\nprint(western_states$NAME)\n\n\n [1] \"Arizona\"    \"Colorado\"   \"Idaho\"      \"Montana\"    \"Nevada\"    \n [6] \"California\" \"New Mexico\" \"Oregon\"     \"Utah\"       \"Washington\"\n[11] \"Wyoming\"   \n\n\n\n\n\n\nTask 3Try it yourself!Solution\n\n\nCalculate the area of each western state using st_area(). The result will be in square meters. Create a new column in your western_states object called area_sqkm that contains the area in square kilometers (Hint: 1 \\(km^2\\) = 1,000,000 \\(m^2\\) ).\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# First, create the 'western_states' object from the previous task\nwestern_states &lt;- us_states %&gt;% \n  filter(REGION == \"West\")\n\n# Now, add the new area column\nwestern_states_with_area &lt;- western_states %&gt;%\n  mutate(area_sqkm = st_area(.) / 1000000)\n\n# View the NAME and the new area column\nprint(western_states_with_area[, c(\"NAME\", \"area_sqkm\")])\n\n\nSimple feature collection with 11 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.7042 ymin: 31.33224 xmax: -102.0422 ymax: 49.00236\nGeodetic CRS:  NAD83\nFirst 10 features:\n         NAME      area_sqkm                       geometry\n1     Arizona 295348.8 [m^2] MULTIPOLYGON (((-114.7196 3...\n2    Colorado 269350.7 [m^2] MULTIPOLYGON (((-109.0501 4...\n3       Idaho 216065.1 [m^2] MULTIPOLYGON (((-116.916 45...\n4     Montana 379804.1 [m^2] MULTIPOLYGON (((-116.0492 4...\n5      Nevada 286105.0 [m^2] MULTIPOLYGON (((-119.9992 4...\n6  California 409575.9 [m^2] MULTIPOLYGON (((-118.6034 3...\n7  New Mexico 314949.2 [m^2] MULTIPOLYGON (((-109.0452 3...\n8      Oregon 250851.5 [m^2] MULTIPOLYGON (((-123.5477 4...\n9        Utah 219662.3 [m^2] MULTIPOLYGON (((-114.0417 4...\n10 Washington 174949.3 [m^2] MULTIPOLYGON (((-122.7699 4...\n\n\n\n\n\n\nTask 4Try it yourself!Solution\n\n\nCreate a simple plot of your western_states object using plot(). Customize it by changing the color of the polygons (col) and their borders (border).\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# First, create the 'western_states' object from task 2\nwestern_states &lt;- us_states %&gt;% \n  filter(REGION == \"West\")\n\nplot(st_geometry(western_states), \n     main = \"Western US States\", \n     col = \"khaki\", \n     border = \"darkgreen\")"
  },
  {
    "objectID": "modules/module1-essentials.html#raster-data-with-terra",
    "href": "modules/module1-essentials.html#raster-data-with-terra",
    "title": "Module 1: Mastering R for Spatial Data",
    "section": "1.3 Raster Data with terra",
    "text": "1.3 Raster Data with terra\n\nWhat is Raster Data?\nWhile vector data uses discrete shapes, raster data represents the world as a continuous grid of cells, or pixels. Each pixel in the grid has a value representing some measured phenomenon. Think of it like a digital photograph.\nEpidemiological examples: Satellite imagery, elevation models, or surfaces showing environmental variables like land surface temperature, precipitation, or vegetation density (NDVI).\n\n\nThe terra Package\nThe terra package is the modern, high-performance engine for raster data in R. It is designed to be easier to use and much faster than its predecessor, the raster package, especially with the very large files common in remote sensing.\n\n\nWorked Example: Exploring Elevation in Uganda\nLet’s download elevation data for Uganda and perform two fundamental raster operations: cropping and masking.\n\n\nShow/Hide Code\n# We will use the sf object for Uganda we can get from the 'world' dataset\nuganda_sf &lt;- world %&gt;% filter(iso_a2 == \"UG\")\n\n# 1. Download Data\n# The geodata package can download various global datasets.\n# We will get elevation data at 30-arcsecond resolution.\ntemp_path &lt;- tempdir()\nelevation_uganda &lt;- geodata::elevation_30s(country = \"UGA\", path = temp_path)\n\n# 2. Inspect the SpatRaster Object\nprint(elevation_uganda)\n\n\nclass       : SpatRaster \nsize        : 720, 696, 1  (nrow, ncol, nlyr)\nresolution  : 0.008333333, 0.008333333  (x, y)\nextent      : 29.4, 35.2, -1.6, 4.4  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : UGA_elv_msk.tif \nname        : UGA_elv_msk \nmin value   :         580 \nmax value   :        4656 \n\n\nShow/Hide Code\n# 3. Basic Plotting\nplot(elevation_uganda, main = \"Elevation of Uganda (Rectangular Extent)\")\nplot(st_geometry(uganda_sf), add = TRUE, border = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nShow/Hide Code\n# 4. Crop and Mask\nelevation_cropped &lt;- crop(elevation_uganda, uganda_sf)\nelevation_masked &lt;- mask(elevation_cropped, uganda_sf)\n\n# 5. Plot the Final Result\nplot(elevation_masked, main = \"Masked Elevation of Uganda (m)\")\nplot(st_geometry(uganda_sf), add = TRUE, border = \"black\", lwd = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Raster Data Manipulation\nLet’s practice these skills by creating a map of the mean annual temperature for Brazil.\n\nTask 1Try it yourself!Solution\n\n\nDownload the global average maximum temperature data from WorldClim at 10-minute resolution. This will be a SpatRaster with 12 layers (one for each month). Calculate the mean across all 12 layers to get a single-layer raster of the annual average. Remember: The data is in °C * 10, so you’ll need to divide the final result by 10.\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\ntemp_path &lt;- tempdir() # Define temp_path if not already defined\nglobal_tmax &lt;- geodata::worldclim_global(var = \"tmax\", res = 10, path = temp_path)\nmean_annual_tmax &lt;- mean(global_tmax)\nmean_annual_tmax_c &lt;- mean_annual_tmax / 10\n\n# Optional: print the final raster object to check it\nprint(mean_annual_tmax_c)\n\n\nclass       : SpatRaster \nsize        : 1080, 2160, 1  (nrow, ncol, nlyr)\nresolution  : 0.1666667, 0.1666667  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        :      mean \nmin value   : -4.970683 \nmax value   :  3.851019 \n\n\n\n\n\n\nTask 2Try it yourself!Solution\n\n\nCreate an sf object for Brazil from the world dataset.\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\nbrazil_sf &lt;- world %&gt;%\n  filter(name_long == \"Brazil\")\n  \nprint(brazil_sf)\n\n\nSimple feature collection with 1 feature and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -73.98724 ymin: -33.76838 xmax: -34.72999 ymax: 5.244486\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 11\n  iso_a2 name_long continent   region_un subregion type  area_km2    pop lifeExp\n* &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 BR     Brazil    South Amer… Americas  South Am… Sove… 8508557. 2.04e8    75.0\n# ℹ 2 more variables: gdpPercap &lt;dbl&gt;, geom &lt;MULTIPOLYGON [°]&gt;\n\n\n\n\n\n\nTask 3Try it yourself!Solution\n\n\nCrop and then mask the global temperature raster (mean_annual_tmax_c) to the exact shape of Brazil.\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Re-create objects from previous tasks to make this chunk self-contained\ntemp_path &lt;- tempdir()\nglobal_tmax &lt;- geodata::worldclim_global(var = \"tmax\", res = 10, path = temp_path)\nmean_annual_tmax_c &lt;- mean(global_tmax) / 10\nbrazil_sf &lt;- world %&gt;% filter(name_long == \"Brazil\")\n\n# Now perform the crop and mask\ntmax_brazil_cropped &lt;- crop(mean_annual_tmax_c, brazil_sf)\ntmax_brazil_masked &lt;- mask(tmax_brazil_cropped, brazil_sf)\n\nprint(tmax_brazil_masked)\n\n\nclass       : SpatRaster \nsize        : 234, 236, 1  (nrow, ncol, nlyr)\nresolution  : 0.1666667, 0.1666667  (x, y)\nextent      : -74, -34.66667, -33.83333, 5.166667  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nname        :     mean \nmin value   : 1.781475 \nmax value   : 3.414175 \n\n\n\n\n\n\nTask 4Try it yourself!Solution\n\n\nPlot your final masked temperature raster for Brazil. Add the country’s border to the plot for context.\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Re-create the necessary objects from previous tasks\ntemp_path &lt;- tempdir()\nglobal_tmax &lt;- geodata::worldclim_global(var = \"tmax\", res = 10, path = temp_path)\nmean_annual_tmax_c &lt;- mean(global_tmax) / 10\nbrazil_sf &lt;- world %&gt;% filter(name_long == \"Brazil\")\ntmax_brazil_masked &lt;- mask(crop(mean_annual_tmax_c, brazil_sf), brazil_sf)\n\n# Now create the plot\nplot(tmax_brazil_masked, main = \"Mean Annual Max Temperature for Brazil (°C)\")\nplot(st_geometry(brazil_sf), add = TRUE, border = \"black\")"
  },
  {
    "objectID": "modules/module1-essentials.html#creating-maps-with-tmap",
    "href": "modules/module1-essentials.html#creating-maps-with-tmap",
    "title": "Module 1: Mastering R for Spatial Data",
    "section": "1.4 Creating Maps with tmap",
    "text": "1.4 Creating Maps with tmap\nWhile plot() is great for quick checks, the tmap package provides a powerful and flexible system for creating publication-quality thematic maps. It uses a “grammar of graphics” approach, similar to ggplot2, where you build a map layer by layer.\nThe main components are: * tm_shape(): Specifies the sf or SpatRaster object to be mapped. * tm_polygons(), tm_lines(), tm_dots(), tm_raster(): Specify how to draw the shape. * tm_layout(), tm_compass(), tm_scale_bar(): Functions to customize the map’s appearance and add map furniture.\nA key feature of tmap is its dual-mode functionality. You can create: 1. Static Maps (tmap_mode(\"plot\")): High-quality images suitable for publications, reports, and presentations. 2. Interactive Maps (tmap_mode(\"view\")): Dynamic maps that you can pan, zoom, and query in a web browser or the RStudio Viewer. This is incredibly useful for data exploration.\n\nWorked Example: Static Maps\nLet’s create a high-quality map showing life expectancy in Africa, highlighting Uganda, and another map showing Uganda’s elevation.\n\n\nShow/Hide Code\n# Set tmap mode to \"plot\" for static maps.\ntmap_mode(\"plot\")\n\n# Map 1: A Choropleth Map of Life Expectancy\n# A choropleth map is one where polygons are colored according to a variable's value.\nmap1 &lt;- tm_shape(world_africa) +\n  tm_polygons(\n    col = \"lifeExp\",                      # The column to map\n    palette = \"plasma\",                   # Color palette\n    style = \"quantile\",                   # How to break data into color bins\n    title = \"Life Expectancy (Years)\"     # Legend title\n    ) +\n  tm_shape(uganda_sf) +                   # Add a new shape layer for Uganda\n  tm_borders(col = \"black\", lwd = 2.5) +  # Draw its borders\n  tm_layout(\n    main.title = \"Life Expectancy in African Nations\",\n    main.title.position = \"center\",\n    legend.position = c(\"left\", \"bottom\"),\n    frame = FALSE, \n    inner.margins = c(0.05, 0.1, 0.05, 0.05)\n    ) +\n  tm_compass(type = \"8star\", position = c(\"right\", \"top\"), size = 3) +\n  tm_scale_bar(position = c(\"right\", \"bottom\"), breaks = c(0, 1000, 2000))\n\n# Map 2: Combining Raster and Vector Data\n# Let's map the masked Uganda elevation raster we created earlier.\nmap2 &lt;- tm_shape(elevation_masked) +\n  tm_raster(\n    title = \"Elevation (m)\"\n    ) +\n  tm_shape(uganda_sf) +\n  tm_borders(lwd = 1.5, col = \"black\") +\n  tm_layout(main.title = \"Elevation of Uganda\")  +\n  tm_compass(type = \"arrow\", position = c(\"left\", \"top\")) +\n  tm_scale_bar()\n\n# Print the maps\nmap1\nmap2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Power of Interactive Mode\nThe real magic of tmap for data exploration comes from its interactive mode. By simply running tmap_mode(\"view\") once, all subsequent tmap plots become interactive Leaflet maps. You can click on features to see their data, zoom in on areas of interest, and switch between different basemaps. This is invaluable for sanity-checking your data and discovering patterns.\nLet’s take our first map and view it interactively.\n\n\nShow/Hide Code\n# Switch to interactive view mode\ntmap_mode(\"view\")\n\n# Re-run the same code for our first map\n# No other changes are needed!\ntm_shape(world_africa) +\n  tm_polygons(\n    fill = \"lifeExp\",\n    # When hovering, show the country name and its life expectancy\n    popup.vars = c(\"Country\" = \"name_long\", \"Life Expectancy\" = \"lifeExp\")\n    ) +\n  tm_shape(uganda_sf) +\n  tm_borders(col = \"black\", lwd = 2.5) +\n  tm_layout(main.title = \"Interactive Map of Life Expectancy in Africa\")\n\n\n\n\n\n\n\nShow/Hide Code\n# --- IMPORTANT ---\n# Switch back to plot mode for the rest of the document\ntmap_mode(\"plot\")\n\n\n\n\n\nExercise 3: Thematic Mapping with tmap\nNow, create your own high-quality map of the United States.\n\nTask 1Try it yourself!Solution\n\n\nUsing the us_states dataset, create a choropleth map showing the total population in 2015 (total_pop_15). Add state borders with tm_borders().\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Ensure us_states data is loaded\ndata(us_states)\n\ntm_shape(us_states) +\n  tm_polygons(col = \"total_pop_15\", title = \"Population (2015)\") +\n  tm_borders(col = \"gray70\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 2Try it yourself!Solution\n\n\nImprove the map from Task 1. * Add a main title. * Change the color palette (try \"YlGnBu\" or \"Reds\"). * Add a compass and a scale bar. * Make the state borders white and thin (lwd = 0.5).\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Ensure us_states data is loaded\ndata(us_states)\n\ntm_shape(us_states) +\n  tm_polygons(\n    col = \"total_pop_15\", \n    title = \"Total Population (2015)\",\n    palette = \"YlGnBu\",\n    style = \"jenks\" # Jenks style is good for skewed data\n    ) +\n  tm_borders(col = \"white\", lwd = 0.5) +\n  tm_layout(\n    main.title = \"Population of the United States, 2015\",\n    frame = FALSE,\n    legend.outside = TRUE\n    ) +\n  tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n  tm_scale_bar(position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "modules/module1-essentials.html#creating-maps-with-ggplot2",
    "href": "modules/module1-essentials.html#creating-maps-with-ggplot2",
    "title": "Module 1: Mastering R for Spatial Data",
    "section": "1.5 Creating Maps with ggplot2",
    "text": "1.5 Creating Maps with ggplot2\nFor those already familiar with the ggplot2 package, its “grammar of graphics” can be extended to create highly customized, professional maps. The sf package integrates directly with ggplot2 through a special geometric layer: geom_sf().\nThe core idea is the same as any other ggplot: you initialize a plot with ggplot(), specify your data, and then add layers. For spatial data, geom_sf() is the primary layer.\n\nWorked Example: Mapping US Population with ggplot2\nLet’s use ggplot2 and geom_sf to create a map of population density for the us_states dataset.\n\n\nShow/Hide Code\n# Load the data if not already in the environment\ndata(us_states)\n\n# We can add a new column for population density right in our pipe\nus_states_plot &lt;- us_states %&gt;%\n  mutate(pop_density = as.numeric(total_pop_15 / AREA))\n\n# Create the map with ggplot2\nggplot(data = us_states_plot) +\n  # The main geometry layer for sf objects\n  geom_sf(aes(fill = pop_density)) + \n  \n  # Customize the color scale\n  scale_fill_viridis_c(\n    trans = \"log10\", # Use a log scale for better visualization of skewed data\n    name = \"Population Density\\n(people / sq. mile, log scale)\"\n    ) +\n  \n  # Add titles and a clean theme\n  labs(\n    title = \"US Population Density by State, 2015\",\n    subtitle = \"Data from the spData package\",\n    caption = \"Map created with ggplot2\"\n  ) +\n  theme_void() + # A minimal theme, good for maps\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nExercise 4: Thematic Mapping with ggplot2\nLet’s use ggplot2 to visualize the economic data in the world_africa dataset we created earlier.\n\nTask 1Try it yourself!Solution\n\n\nUsing the world_africa object, create a simple map of the African continent using ggplot() and geom_sf().\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Assuming 'world_africa' was created in section 1.2\nggplot(data = world_africa) +\n  geom_sf() +\n  ggtitle(\"Map of Africa\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 2Try it yourself!Solution\n\n\nMap the gdpPercap (GDP per capita) variable to the fill aesthetic of the polygons.\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\nggplot(data = world_africa) +\n  geom_sf(aes(fill = gdpPercap))\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 3Try it yourself!Solution\n\n\nImprove your map. * Use scale_fill_viridis_c() for a better color scale and add a title to the legend. * Add a main title and a subtitle to the map using labs(). * Apply theme_bw() for a cleaner look. * Add a black border layer for Uganda (uganda_sf) on top of the other countries. (Hint: Use a second geom_sf() call, but set fill = NA so it doesn’t cover the colors).\n\n\n\n\nShow/Hide Code\n# Your code here\n\n\n\n\n\n\nShow/Hide Code\n# Assuming 'world_africa' and 'uganda_sf' are loaded\nggplot() +\n  # Layer 1: African countries colored by GDP\n  geom_sf(data = world_africa, aes(fill = gdpPercap)) +\n  \n  # Layer 2: Uganda outline\n  geom_sf(data = uganda_sf, fill = NA, color = \"red\", linewidth = 0.8) +\n  \n  # Customize scales and labels\n  scale_fill_viridis_c(name = \"GDP per Capita (USD)\") +\n  labs(\n    title = \"GDP Per Capita in Africa\",\n    subtitle = \"Highlighting Uganda\",\n    caption = \"Source: spData package\"\n  ) +\n  theme_bw()"
  }
]